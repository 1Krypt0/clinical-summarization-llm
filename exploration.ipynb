{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "This notebook is split into 3 main parts, related to 3 main stages of development\n",
    "\n",
    "1. General Data Exploration - The initial part was dedicated to gain an overall understanding of the data and how it is structured.\n",
    "2. Common Headers - The second part was written to extract the most common headers, so zero-shot models can have a structure to work with.\n",
    "3. Additional Constrains - As the dataset was giving Out Of Memory errors, I had to reduce the dataset to a smaller size, and this part was written to figure out a cutoff point that would be reasonable.\n",
    "4. Final Data Exploration - After reducing the dataset, I conduct a more in-depth exploration of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - General Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/NOTEEVENTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = data.groupby(\"CATEGORY\").size().reset_index().rename(columns={0: \"count\"})\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_summaries = data[data[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "discharge_summaries = discharge_summaries[\n",
    "    discharge_summaries[\"DESCRIPTION\"] == \"Report\"\n",
    "]\n",
    "discharge_summaries = discharge_summaries[\n",
    "    discharge_summaries[\"TEXT\"].map(len) < 16000\n",
    "]  # 16000 is the approximately the context window for GPT-3.5\n",
    "\n",
    "discharge_summaries[\"TEXT\"].map(len).hist(bins=100)\n",
    "sample = discharge_summaries[discharge_summaries[\"TEXT\"].map(len) == 5000]\n",
    "\n",
    "# Using print to format the output\n",
    "print(sample.iloc[0][\"TEXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_patient = data.sample()[\"SUBJECT_ID\"]\n",
    "random_patient = 99082  # Fix a patient for reproducibility\n",
    "\n",
    "notes = data[data[\"SUBJECT_ID\"] == random_patient]\n",
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = notes[notes[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "print(summary.iloc[0][\"TEXT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Common Headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what the most common headings are in the discharge summaries\n",
    "data = pd.read_csv(\"./data/single-discharge-8k.csv\")\n",
    "data = data[data[\"CATEGORY\"] == \"Discharge summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "headings = {}\n",
    "\n",
    "regex = r\"^.+:\\s\"\n",
    "\n",
    "regex = re.compile(regex, re.MULTILINE)\n",
    "\n",
    "for text in data[\"TEXT\"]:\n",
    "    text = text.lower()\n",
    "    matches = regex.findall(text)\n",
    "    for match in matches:\n",
    "        match = re.sub(r\":\\s\", \":\", match)\n",
    "        if match not in headings:\n",
    "            headings[match] = 0\n",
    "        headings[match] += 1\n",
    "\n",
    "headings, len(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the most common headings and show the top 20\n",
    "\n",
    "sorted_headings = sorted(headings.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# We eliminate the first because it is standard to all discharge summaries\n",
    "sorted_headings[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Additional Constrains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/single-discharge-8k-test-formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sizes of the notes\n",
    "\n",
    "data[\"notes\"].map(len).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-1.1-7b-it\",\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert clinical assistant. You will receive a collection of clinical notes. You will summarize them in the style of a discharge summary.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_testing_prompt_gemma(\n",
    "    notes: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"<start_of_turn>user {system_prompt}\n",
    "\n",
    "### Input:\n",
    "\n",
    "{notes.strip()}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "tokens = data[\"notes\"].map(generate_testing_prompt_gemma)\n",
    "tokens = tokens.map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest = tokens.map(len).idxmax()\n",
    "len(tokens[biggest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Final Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about note size and distribution on the dataset and per admission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the notes in each format (original and ours)\n",
    "data_original = pd.read_csv(\"./data/NOTEEVENTS.csv\")\n",
    "data_train = pd.read_csv(\"./data/single-discharge-7.6k-train-formatted.csv\")\n",
    "data_test = pd.read_csv(\"./data/single-discharge-7.6k-test-formatted.csv\")\n",
    "data_together = pd.concat([data_train, data_test])\n",
    "\n",
    "del data_train, data_test\n",
    "\n",
    "# Add our dataset but before aggregation\n",
    "all = pd.read_csv(\"./data/single-discharge-all.csv\")\n",
    "\n",
    "# Get only the notes from all that are in our dataset\n",
    "admissions = data_together[\"admission\"].unique().tolist()\n",
    "\n",
    "all_admissions = all[all[\"HADM_ID\"].isin(admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of notes over the entire dataset\n",
    "together_notes_len = data_together[\"notes\"].map(nltk.word_tokenize).map(len)\n",
    "together_summary_len = data_together[\"summary\"].map(nltk.word_tokenize).map(len)\n",
    "\n",
    "data_together_len = pd.DataFrame(\n",
    "    {\"notes\": together_notes_len, \"summary\": together_summary_len}\n",
    ")\n",
    "\n",
    "del together_notes_len, together_summary_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of notes over our dataset\n",
    "all_admissions[\"len\"] = all_admissions[\"TEXT\"].map(nltk.word_tokenize).map(len)\n",
    "all_admissions[\"sen\"] = all_admissions[\"TEXT\"].map(nltk.sent_tokenize).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "data_original[\"len\"] = data_original[\"TEXT\"].map(nltk.word_tokenize).map(len)\n",
    "data_original[\"sen\"] = data_original[\"TEXT\"].map(nltk.sent_tokenize).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate MIMIC-III notes by category\n",
    "data_original_notes = data_original[data_original[\"CATEGORY\"] != \"Discharge summary\"]\n",
    "data_original_summary = data_original[data_original[\"CATEGORY\"] == \"Discharge summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine notes and summary to plot their size together\n",
    "ax = sns.histplot(\n",
    "    data=data_together_len,\n",
    "    multiple=\"dodge\",\n",
    "    binwidth=100,\n",
    ")\n",
    "ax.set(ylabel=\"Amount of Documents\", xlabel=\"\")\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "data_original_len = pd.DataFrame(\n",
    "    {\"notes\": data_original_notes[\"len\"], \"summary\": data_original_summary[\"len\"]}\n",
    ")\n",
    "ax = sns.histplot(\n",
    "    data=data_original_len,\n",
    "    multiple=\"dodge\",\n",
    ")\n",
    "ax.set(ylabel=\"Amount of Documents\", xlabel=\"\")\n",
    "ax.grid(axis=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage of notes by category in our dataset\n",
    "plt.xticks(rotation=90)\n",
    "order = all_admissions[\"CATEGORY\"].value_counts().sort_values(ascending=False).index\n",
    "ax = sns.countplot(\n",
    "    data=all_admissions,\n",
    "    x=\"CATEGORY\",\n",
    "    order=order,\n",
    "    stat=\"percent\",\n",
    ")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "plt.xticks(rotation=90)\n",
    "order = data_original[\"CATEGORY\"].value_counts().sort_values(ascending=False).index\n",
    "ax = sns.countplot(\n",
    "    data=data_original,\n",
    "    x=\"CATEGORY\",\n",
    "    order=order,\n",
    "    stat=\"percent\",\n",
    ")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot of type and number of notes per admission\n",
    "notes_per_category_all = (\n",
    "    all_admissions.groupby([\"CATEGORY\", \"HADM_ID\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"note_count\")\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "ax = sns.boxplot(\n",
    "    data=notes_per_category_all,\n",
    "    x=\"CATEGORY\",\n",
    "    y=\"note_count\",\n",
    ")\n",
    "ax.set(xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "notes_per_category_original = (\n",
    "    data_original.groupby([\"CATEGORY\", \"HADM_ID\"]).size().reset_index(name=\"note_count\")\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "ax = sns.boxplot(\n",
    "    data=notes_per_category_original,\n",
    "    x=\"CATEGORY\",\n",
    "    y=\"note_count\",\n",
    "    log_scale=True,\n",
    ")\n",
    "ax.set(xlabel=\"\", ylabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average amount of words per note type on our dataset\n",
    "plt.xticks(rotation=90)\n",
    "avg_len_together = all_admissions.groupby(\"CATEGORY\")[\"len\"].mean().reset_index()\n",
    "ax = sns.barplot(\n",
    "    data=avg_len_together,\n",
    "    x=\"CATEGORY\",\n",
    "    y=\"len\",\n",
    ")\n",
    "ax.bar_label(ax.containers[0], fontsize=10, fmt=\"%.0f\")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "plt.xticks(rotation=90)\n",
    "ax = sns.barplot(\n",
    "    data=data_original,\n",
    "    x=\"CATEGORY\",\n",
    "    y=\"len\",\n",
    "    errorbar=None,\n",
    ")\n",
    "ax.bar_label(ax.containers[0], fontsize=10, fmt=\"%.0f\")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average amount of sentences per note type on our dataset\n",
    "avg_sen_together = all_admissions.groupby(\"CATEGORY\")[\"sen\"].mean().reset_index()\n",
    "plt.xticks(rotation=90)\n",
    "ax = sns.barplot(data=avg_sen_together, x=\"CATEGORY\", y=\"sen\")\n",
    "ax.bar_label(ax.containers[0], fontsize=10, fmt=\"%.0f\")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for MIMIC-III dataset\n",
    "plt.xticks(rotation=90)\n",
    "ax = sns.barplot(data=data_original, x=\"CATEGORY\", y=\"sen\", errorbar=None)\n",
    "ax.bar_label(ax.containers[0], fontsize=10, fmt=\"%.0f\")\n",
    "ax.set(ylabel=\"\", xlabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine most common words in the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection frequency of words in the notes\n",
    "import string\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    text = \" \".join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_cf(data):\n",
    "    cf = {}\n",
    "    for entry in data:\n",
    "        if entry == \"\":\n",
    "            continue\n",
    "        for word in entry.split(\" \"):\n",
    "            if word in cf:\n",
    "                cf[word] += 1\n",
    "            else:\n",
    "                cf[word] = 1\n",
    "    return cf\n",
    "\n",
    "\n",
    "notes_words = data_together[\"notes\"].apply(process_text)\n",
    "summary_words = data_together[\"summary\"].apply(process_text)\n",
    "\n",
    "notes_cf = calculate_cf(notes_words)\n",
    "summary_cf = calculate_cf(summary_words)\n",
    "\n",
    "notes_cf = {\n",
    "    k: v for k, v in sorted(notes_cf.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "summary_cf = {\n",
    "    k: v for k, v in sorted(summary_cf.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "\n",
    "list(notes_cf.items())[:10], list(summary_cf.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "notes_words = notes_words.tolist()\n",
    "\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "document_term_matrix = count_vectorizer.fit_transform(notes_words)\n",
    "notes_frequency = document_term_matrix.sum(axis=0)\n",
    "\n",
    "notes_frequency = pd.Series(\n",
    "    notes_frequency.A1, index=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "notes_frequency = notes_frequency.sort_values(ascending=False)\n",
    "\n",
    "summary_words = summary_words.tolist()\n",
    "\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "document_term_matrix = count_vectorizer.fit_transform(summary_words)\n",
    "summary_frequency = document_term_matrix.sum(axis=0)\n",
    "\n",
    "summary_frequency = pd.Series(\n",
    "    summary_frequency.A1, index=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "summary_frequency = summary_frequency.sort_values(ascending=False)\n",
    "\n",
    "notes_frequency[:10], summary_frequency[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine information about the dates used (not useful since they are random dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_admissions[\"year\"] = all_admissions[\"CHARTDATE\"].map(lambda x: x.split(\"-\")[0])\n",
    "count_per_year = all_admissions.groupby(\"year\").size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26, 6))\n",
    "plt.xticks(rotation=90)\n",
    "sns.lineplot(data=count_per_year, x=\"year\", y=\"count\", marker=\"o\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
