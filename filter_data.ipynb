{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate our Dataset\n",
    "\n",
    "We need to filter out the data that does not matter to us, and those that exceed the limits we have set for this experiment.\n",
    "\n",
    "We will start by removing any admission with corrections to their discharge summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/NOTEEVENTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, drop all rows with errors flagged\n",
    "errors = data[\"ISERROR\"].value_counts()\n",
    "print(errors)\n",
    "\n",
    "data = data[data[\"ISERROR\"].isna()]\n",
    "data = data.drop(columns=[\"ISERROR\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = data[data[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "summaries = summaries.drop_duplicates(subset=\"HADM_ID\", keep=False)\n",
    "\n",
    "admissions = summaries[\"HADM_ID\"].unique()\n",
    "len(admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"HADM_ID\"].isin(admissions)]\n",
    "\n",
    "data.to_csv(\"./data/single-discharge-all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will handle long text sequences. We will cut out any set of notes whose total amount of tokens exceeds 7942 (which is 500 tokens less than 8192, the context window for Mistral)\n",
    "\n",
    "Due to memory limitations, this should be run separately from the first few steps, or you risk going out of memory yet again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/single-discharge-all.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = data[\"HADM_ID\"].unique()\n",
    "len(admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SentencePiece tokenizer to assess the encoding abilities\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceeding = []\n",
    "total = 0\n",
    "total_exceeding = 0\n",
    "\n",
    "TOKEN_LIMIT = 7942\n",
    "\n",
    "for admission in admissions:\n",
    "    notes = data[data[\"HADM_ID\"] == admission]\n",
    "    notes = notes[notes[\"CATEGORY\"] != \"Discharge summary\"]\n",
    "    notes = notes[\"TEXT\"].tolist()\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    for note in notes:\n",
    "        text += note + \"\\n\"\n",
    "\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "    if len(tokens) > TOKEN_LIMIT:\n",
    "        exceeding.append(admission)\n",
    "        total_exceeding += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "    if total % 1000 == 0:\n",
    "        print(f\"Processed {total} admissions, {total_exceeding} exceeding\")\n",
    "\n",
    "print(f\"Total admissions: {total}, exceeding: {total_exceeding}\")\n",
    "admissions = [admission for admission in admissions if admission not in exceeding]\n",
    "len(admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"HADM_ID\"].isin(admissions)]\n",
    "data.to_csv(\"./data/single-discharge-8k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove all the examples that have no notes associated with them besides the dishcarge summary. These would be of no help to our program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/single-discharge-8k.csv\")\n",
    "\n",
    "admissions = data[\"HADM_ID\"].unique()\n",
    "\n",
    "empty = []\n",
    "\n",
    "for admission in admissions:\n",
    "    notes = data[data[\"HADM_ID\"] == admission]\n",
    "    notes = notes[notes[\"CATEGORY\"] != \"Discharge summary\"]\n",
    "    notes = notes[\"TEXT\"].tolist()\n",
    "\n",
    "    if len(notes) == 0:\n",
    "        empty.append(admission)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Amount of admissions before: {len(admissions)}\")\n",
    "admissions = [admission for admission in admissions if admission not in empty]\n",
    "print(f\"Amount of admissions after: {len(admissions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"HADM_ID\"].isin(admissions)]\n",
    "\n",
    "data.to_csv(\"./data/single-discharge-8k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to separate these datasets into a training and testing set. Of the 30k, 1k will be reserved for testing, and the rest will be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = pd.read_csv(\"./data/single-discharge-8k.csv\")\n",
    "\n",
    "admissions = data[\"HADM_ID\"].unique()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "random.shuffle(admissions)\n",
    "\n",
    "test = admissions[:1000]\n",
    "\n",
    "train = admissions[1000:]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data[\"HADM_ID\"].isin(train)]\n",
    "test_data = data[data[\"HADM_ID\"].isin(test)]\n",
    "\n",
    "train_data.to_csv(\"./data/single-discharge-8k-train.csv\", index=False)\n",
    "test_data.to_csv(\"./data/single-discharge-8k-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extra step for the one-shot approaches, we will retrieve one sample from the training set to serve as our guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 101648.0  # obtained via random.choice()\n",
    "\n",
    "sample_notes = data[data[\"HADM_ID\"] == sample]\n",
    "sample_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the notes and the discharge summary\n",
    "\n",
    "summary = sample_notes[sample_notes[\"CATEGORY\"] == \"Discharge summary\"].iloc[0][\"TEXT\"]\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = sample_notes[sample_notes[\"CATEGORY\"] != \"Discharge summary\"]\n",
    "notes = notes[\"TEXT\"].tolist()\n",
    "notes = \"\\n\".join(notes)\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "After experimenting with the original 8k dataset, it proved to be too much for our hardware to handle. As such, we were forced to reduce the maximum size even further, so each sample can fit into the memory. We will set the maximum size to 7600 tokens, to not cut out too much data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/single-discharge-8k-train-formatted.csv\")\n",
    "data_test = pd.read_csv(\"./data/single-discharge-8k-test-formatted.csv\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-1.1-7b-it\",\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert clinical assistant. You will receive a collection of clinical notes. You will summarize them in the style of a discharge summary.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_training_prompt_gemma(\n",
    "    notes: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"<start_of_turn>user {system_prompt}\n",
    "\n",
    "### Input:\n",
    "\n",
    "{notes.strip()}\n",
    "\n",
    "### Summary:\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{summary}\n",
    "<end_of_turn>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "formatted_data = data.apply(\n",
    "    lambda row: generate_training_prompt_gemma(\n",
    "        row[\"notes\"], row[\"summary\"], DEFAULT_SYSTEM_PROMPT\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = []\n",
    "total = 0\n",
    "for entry in formatted_data:\n",
    "    if len(tokenizer(entry, return_tensors=\"pt\")[\"input_ids\"][0]) > 7600:\n",
    "        big.append(entry)\n",
    "    total += 1\n",
    "\n",
    "    if total % 1000 == 0:\n",
    "        print(f\"Processed {total} entries, {len(big)} big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testing_prompt_gemma(\n",
    "    notes: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"<start_of_turn>user {system_prompt}\n",
    "\n",
    "### Input:\n",
    "\n",
    "{notes.strip()}\n",
    "\n",
    "### Summary:\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "formatted_data_test = data_test.apply(\n",
    "    lambda row: generate_testing_prompt_gemma(row[\"notes\"], DEFAULT_SYSTEM_PROMPT),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_test = []\n",
    "total = 0\n",
    "for entry in formatted_data_test:\n",
    "    if len(tokenizer(entry, return_tensors=\"pt\")[\"input_ids\"][0]) > 7600:\n",
    "        big_test.append(entry)\n",
    "    total += 1\n",
    "\n",
    "    if total % 1000 == 0:\n",
    "        print(f\"Processed {total} entries, {len(big_test)} big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data in order to remove the big entries\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    if (\n",
    "        generate_training_prompt_gemma(\n",
    "            row[\"notes\"], row[\"summary\"], DEFAULT_SYSTEM_PROMPT\n",
    "        )\n",
    "        in big\n",
    "    ):\n",
    "        data.drop(i, inplace=True)\n",
    "\n",
    "for i, row in data_test.iterrows():\n",
    "    if generate_testing_prompt_gemma(row[\"notes\"], DEFAULT_SYSTEM_PROMPT) in big_test:\n",
    "        data_test.drop(i, inplace=True)\n",
    "\n",
    "len(data), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/single-discharge-7.6k-train-formatted.csv\", index=False)\n",
    "data_test.to_csv(\"./data/single-discharge-7.6k-test-formatted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
